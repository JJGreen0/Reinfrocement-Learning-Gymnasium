{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49227597-1e10-452e-ac53-b6395e3494da",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install stable-baselines3[extra] gym[atari] ale-py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61e8847e-3dd6-4275-8765-cacc7f960bcf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import os\n",
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.common.env_util import make_atari_env\n",
    "from stable_baselines3.common.vec_env import VecFrameStack\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import time\n",
    "\n",
    "# Step 1: Create the environment\n",
    "# Replace 'BreakoutNoFrameskip-v4' with the Atari game you're interested in\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ef92627-3e2b-42d4-bec5-073b82b790cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env_name = \"ALE/Alien-v5\"\n",
    "env = make_atari_env(env_name, n_envs=1, seed=0)\n",
    "# Frame-stacking with 4 frames\n",
    "env = VecFrameStack(env, n_stack=12)\n",
    "\n",
    "# Step 2: Initialize the agent\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a12c858-a4a1-488d-885c-713924e705b0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    }
   ],
   "source": [
    "model = A2C('CnnPolicy', env, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "53c31c7b-2f6a-44ee-92ac-a2a46cff0d09",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 581      |\n",
      "|    ep_rew_mean        | 264      |\n",
      "| time/                 |          |\n",
      "|    fps                | 45       |\n",
      "|    iterations         | 100      |\n",
      "|    time_elapsed       | 131      |\n",
      "|    total_timesteps    | 6000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.25    |\n",
      "|    explained_variance | 0.027    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 124      |\n",
      "|    policy_loss        | 1.22     |\n",
      "|    value_loss         | 5.29     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 588      |\n",
      "|    ep_rew_mean        | 262      |\n",
      "| time/                 |          |\n",
      "|    fps                | 45       |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 261      |\n",
      "|    total_timesteps    | 12000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.95    |\n",
      "|    explained_variance | 0.216    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 224      |\n",
      "|    policy_loss        | -1.64    |\n",
      "|    value_loss         | 3.87     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 605      |\n",
      "|    ep_rew_mean        | 268      |\n",
      "| time/                 |          |\n",
      "|    fps                | 46       |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 384      |\n",
      "|    total_timesteps    | 18000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.81    |\n",
      "|    explained_variance | 0.579    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 324      |\n",
      "|    policy_loss        | -1.1     |\n",
      "|    value_loss         | 3.47     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 596      |\n",
      "|    ep_rew_mean        | 272      |\n",
      "| time/                 |          |\n",
      "|    fps                | 47       |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 508      |\n",
      "|    total_timesteps    | 24000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2       |\n",
      "|    explained_variance | 0.697    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 424      |\n",
      "|    policy_loss        | 0.0533   |\n",
      "|    value_loss         | 1.48     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 593      |\n",
      "|    ep_rew_mean        | 272      |\n",
      "| time/                 |          |\n",
      "|    fps                | 46       |\n",
      "|    iterations         | 500      |\n",
      "|    time_elapsed       | 639      |\n",
      "|    total_timesteps    | 30000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.65    |\n",
      "|    explained_variance | 0.388    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 524      |\n",
      "|    policy_loss        | -0.507   |\n",
      "|    value_loss         | 4.28     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 590      |\n",
      "|    ep_rew_mean        | 293      |\n",
      "| time/                 |          |\n",
      "|    fps                | 47       |\n",
      "|    iterations         | 600      |\n",
      "|    time_elapsed       | 765      |\n",
      "|    total_timesteps    | 36000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.61    |\n",
      "|    explained_variance | 0.754    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 624      |\n",
      "|    policy_loss        | -0.51    |\n",
      "|    value_loss         | 1.97     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 576      |\n",
      "|    ep_rew_mean        | 330      |\n",
      "| time/                 |          |\n",
      "|    fps                | 46       |\n",
      "|    iterations         | 700      |\n",
      "|    time_elapsed       | 894      |\n",
      "|    total_timesteps    | 42000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.38    |\n",
      "|    explained_variance | 0.418    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 724      |\n",
      "|    policy_loss        | 0.932    |\n",
      "|    value_loss         | 1.23     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 574      |\n",
      "|    ep_rew_mean        | 348      |\n",
      "| time/                 |          |\n",
      "|    fps                | 47       |\n",
      "|    iterations         | 800      |\n",
      "|    time_elapsed       | 1020     |\n",
      "|    total_timesteps    | 48000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.47    |\n",
      "|    explained_variance | 0.263    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 824      |\n",
      "|    policy_loss        | 0.747    |\n",
      "|    value_loss         | 1.97     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 604      |\n",
      "|    ep_rew_mean        | 353      |\n",
      "| time/                 |          |\n",
      "|    fps                | 46       |\n",
      "|    iterations         | 900      |\n",
      "|    time_elapsed       | 1153     |\n",
      "|    total_timesteps    | 54000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.14    |\n",
      "|    explained_variance | 0.773    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 924      |\n",
      "|    policy_loss        | -0.351   |\n",
      "|    value_loss         | 1.07     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 602      |\n",
      "|    ep_rew_mean        | 364      |\n",
      "| time/                 |          |\n",
      "|    fps                | 47       |\n",
      "|    iterations         | 1000     |\n",
      "|    time_elapsed       | 1274     |\n",
      "|    total_timesteps    | 60000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.09    |\n",
      "|    explained_variance | 0.39     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1024     |\n",
      "|    policy_loss        | 0.556    |\n",
      "|    value_loss         | 2.48     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 608      |\n",
      "|    ep_rew_mean        | 360      |\n",
      "| time/                 |          |\n",
      "|    fps                | 47       |\n",
      "|    iterations         | 1100     |\n",
      "|    time_elapsed       | 1400     |\n",
      "|    total_timesteps    | 66000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.38    |\n",
      "|    explained_variance | 0.851    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1124     |\n",
      "|    policy_loss        | 1.3      |\n",
      "|    value_loss         | 1.04     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 641      |\n",
      "|    ep_rew_mean        | 340      |\n",
      "| time/                 |          |\n",
      "|    fps                | 47       |\n",
      "|    iterations         | 1200     |\n",
      "|    time_elapsed       | 1530     |\n",
      "|    total_timesteps    | 72000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.64    |\n",
      "|    explained_variance | 0.148    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1224     |\n",
      "|    policy_loss        | 0.0222   |\n",
      "|    value_loss         | 3.35     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 608      |\n",
      "|    ep_rew_mean        | 330      |\n",
      "| time/                 |          |\n",
      "|    fps                | 33       |\n",
      "|    iterations         | 1300     |\n",
      "|    time_elapsed       | 2306     |\n",
      "|    total_timesteps    | 78000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0.724    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1324     |\n",
      "|    policy_loss        | -0.873   |\n",
      "|    value_loss         | 1.62     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 510      |\n",
      "|    ep_rew_mean        | 309      |\n",
      "| time/                 |          |\n",
      "|    fps                | 34       |\n",
      "|    iterations         | 1400     |\n",
      "|    time_elapsed       | 2432     |\n",
      "|    total_timesteps    | 84000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.13    |\n",
      "|    explained_variance | 0.89     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1424     |\n",
      "|    policy_loss        | 0.62     |\n",
      "|    value_loss         | 1.05     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 500      |\n",
      "|    ep_rew_mean        | 302      |\n",
      "| time/                 |          |\n",
      "|    fps                | 35       |\n",
      "|    iterations         | 1500     |\n",
      "|    time_elapsed       | 2555     |\n",
      "|    total_timesteps    | 90000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.3     |\n",
      "|    explained_variance | 0.848    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1524     |\n",
      "|    policy_loss        | 0.146    |\n",
      "|    value_loss         | 1.11     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 481      |\n",
      "|    ep_rew_mean        | 309      |\n",
      "| time/                 |          |\n",
      "|    fps                | 35       |\n",
      "|    iterations         | 1600     |\n",
      "|    time_elapsed       | 2676     |\n",
      "|    total_timesteps    | 96000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.47    |\n",
      "|    explained_variance | 0.897    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1624     |\n",
      "|    policy_loss        | 0.425    |\n",
      "|    value_loss         | 0.889    |\n",
      "------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.a2c.a2c.A2C at 0x1e3aa4acc50>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 3: Train the agent\n",
    "time_steps = 100000  # Adjust the number of time steps for training\n",
    "model.learn(total_timesteps=int(time_steps))\n",
    "\n",
    "# Step 4: Save the mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e1711aa-550c-4384-91c1-658be5bd059d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward: 166.0 +/- 42.23742416388575\n"
     ]
    }
   ],
   "source": [
    "mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=10)\n",
    "print(f\"Mean reward: {mean_reward} +/- {std_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "36959165-73b3-4f2b-8947-e9c9cb7f48c6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    }
   ],
   "source": [
    "a2c_path = os.path.join('Training', 'Saved Models', 'a2c_boxer')\n",
    "model = A2C.load(a2c_path, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7cec957f-e605-4664-90a4-e017059628b3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1000\u001b[39m):  \u001b[38;5;66;03m# Adjust the range for longer play\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     action, _states \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(obs, deterministic\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 4\u001b[0m     obs, rewards, dones, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m      5\u001b[0m     env\u001b[38;5;241m.\u001b[39mrender(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.05\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\base_vec_env.py:206\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;124;03mStep the environments with the given action\u001b[39;00m\n\u001b[0;32m    201\u001b[0m \n\u001b[0;32m    202\u001b[0m \u001b[38;5;124;03m:param actions: the action\u001b[39;00m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;124;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[1;32m--> 206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_wait()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\vec_frame_stack.py:33\u001b[0m, in \u001b[0;36mVecFrameStack.step_wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_wait\u001b[39m(\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     32\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Union[np\u001b[38;5;241m.\u001b[39mndarray, Dict[\u001b[38;5;28mstr\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray]], np\u001b[38;5;241m.\u001b[39mndarray, np\u001b[38;5;241m.\u001b[39mndarray, List[Dict[\u001b[38;5;28mstr\u001b[39m, Any]],]:\n\u001b[1;32m---> 33\u001b[0m     observations, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvenv\u001b[38;5;241m.\u001b[39mstep_wait()\n\u001b[0;32m     34\u001b[0m     observations, infos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstacked_obs\u001b[38;5;241m.\u001b[39mupdate(observations, dones, infos)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m observations, rewards, dones, infos\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py:58\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;66;03m# Avoid circular imports\u001b[39;00m\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs):\n\u001b[1;32m---> 58\u001b[0m         obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_rews[env_idx], terminated, truncated, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvs[env_idx]\u001b[38;5;241m.\u001b[39mstep(\n\u001b[0;32m     59\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactions[env_idx]\n\u001b[0;32m     60\u001b[0m         )\n\u001b[0;32m     61\u001b[0m         \u001b[38;5;66;03m# convert to SB3 VecEnv api\u001b[39;00m\n\u001b[0;32m     62\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones[env_idx] \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\gymnasium\\core.py:461\u001b[0m, in \u001b[0;36mWrapper.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    457\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\n\u001b[0;32m    458\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[0;32m    459\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[0;32m    460\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 461\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\gymnasium\\core.py:555\u001b[0m, in \u001b[0;36mRewardWrapper.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    551\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\n\u001b[0;32m    552\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: ActType\n\u001b[0;32m    553\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[ObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[0;32m    554\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Modifies the :attr:`env` :meth:`step` reward using :meth:`self.reward`.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 555\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m    556\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m observation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreward(reward), terminated, truncated, info\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\gymnasium\\core.py:522\u001b[0m, in \u001b[0;36mObservationWrapper.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    518\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\n\u001b[0;32m    519\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: ActType\n\u001b[0;32m    520\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[0;32m    521\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Modifies the :attr:`env` after calling :meth:`step` using :meth:`self.observation` on the returned observations.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 522\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m    523\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation(observation), reward, terminated, truncated, info\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\gymnasium\\core.py:461\u001b[0m, in \u001b[0;36mWrapper.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    457\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\n\u001b[0;32m    458\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[0;32m    459\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[0;32m    460\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 461\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\stable_baselines3\\common\\atari_wrappers.py:112\u001b[0m, in \u001b[0;36mEpisodicLifeEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AtariStepReturn:\n\u001b[1;32m--> 112\u001b[0m     obs, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwas_real_done \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;66;03m# check current lives, make loss of life terminal,\u001b[39;00m\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;66;03m# then update lives to handle bonus lives\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\stable_baselines3\\common\\atari_wrappers.py:189\u001b[0m, in \u001b[0;36mMaxAndSkipEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    186\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;66;03m# Note that the observation on the done=True frame\u001b[39;00m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;66;03m# doesn't matter\u001b[39;00m\n\u001b[1;32m--> 189\u001b[0m max_frame \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_obs_buffer\u001b[38;5;241m.\u001b[39mmax(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m max_frame, total_reward, terminated, truncated, info\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\numpy\\core\\_methods.py:41\u001b[0m, in \u001b[0;36m_amax\u001b[1;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_amax\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     40\u001b[0m           initial\u001b[38;5;241m=\u001b[39m_NoValue, where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m---> 41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m umr_maximum(a, axis, \u001b[38;5;28;01mNone\u001b[39;00m, out, keepdims, initial, where)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "for _ in range(1000):  # Adjust the range for longer play\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, rewards, dones, info = env.step(action)\n",
    "    env.render('human')\n",
    "    time.sleep(0.05)\n",
    "    if dones:\n",
    "        obs = env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f1c3ca-06f3-4c34-b5a0-3b4623051874",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d15378-0656-4d65-9b1b-e1ce4a670c83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
